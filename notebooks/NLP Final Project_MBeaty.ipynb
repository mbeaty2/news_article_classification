{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "## Marissa Beaty\n",
    "\n",
    "For my NLP Final Project, I will be using a \"True\" and \"Fake\" news dataset to topic model and train an LSTM classifier. The goal of this project is to showcase my understanding of topic modelling, but also show an early introduction to what can be done with this type of data in terms of classification. For the purpose of this project, the topic modeling and classifying will be done using the news article titles. \n",
    "\n",
    "The results of this training (i.e. accuracy, top words, etc.) will be discussed under the results section of my paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing necessary packages, taken from NLP Lecture 5.1.2\n",
    "import numpy as np  \n",
    "from keras.preprocessing import sequence   \n",
    "from keras.models import Sequential        \n",
    "from keras.layers import Dense, Dropout, Activation    \n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing and Reading the Data into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes our two separate datasets, labels them based on their fake and true status, reduces their size, and combines them into one dataframe that is saved and read back into this jupyter notebook. Once the data has been saved, I do a quick check to confirm the distribution of true versus fake articles is near equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the fake and true news articles into dataframes\n",
    "full_fake_df = pd.read_csv(\"Data/Fake.csv\")\n",
    "full_true_df = pd.read_csv(\"Data/True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the size of the full dataframes by taking a random sample\n",
    "fake_df = full_fake_df.sample(frac=0.05, replace=True, random_state=1)\n",
    "true_df = full_true_df.sample(frac=0.05, replace=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column in this dataframe called \"label\" and give all fake articles a label value of 0 and all true a label of 1.\n",
    "fake_df.loc[:, \"label\"] = 0\n",
    "true_df.loc[:, \"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-b4c258186164>:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  news_dataframe = true_df.append(fake_df, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#create a new dataframe with all fake and true news articles\n",
    "news_dataframe = true_df.append(fake_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save that new dataframe as a .csv\n",
    "file_name = \"combined_news_dataset\"\n",
    "news_dataframe.to_csv(\"data/\" + file_name + \".csv\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the new dataset back into the jupyter notebook\n",
    "combined_news_dataset = pd.read_csv(\"Data/\" + file_name + \".csv\",sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1174\n",
       "1    1071\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the distribution of fake to true news articles\n",
    "combined_news_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I define a tokenizer to clean up my data. First, I replace a term to better fit the dataset. Then I remove all special characters, and finally remove all stop words before saving the new set of words in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marissabeaty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import stop words from the NLTK library: https://www.nltk.org/search.html?q=stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a tokenizer to clean up and tokenize my data\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    #changing one term so the tokenizer does not remove important information\n",
    "    article_titles = combined_news_dataset[\"title\"].str.replace(\"U.S.\", \"unitedstates\", case = False)\n",
    "    \n",
    "    #removing all stop characters and adding the regexed titles back into a new list\n",
    "    stop_char = \"[^A-Za-z0-9]+\"\n",
    "\n",
    "    regexed_titles = []\n",
    "    for title in article_titles:\n",
    "        regexed_articles = re.sub(stop_char, ' ', str(title).lower()).strip()\n",
    "        regexed_titles.append(regexed_articles)\n",
    "        \n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    #removing all stop words and putting them into a new list\n",
    "    go_words = []\n",
    "    for titles in regexed_titles:\n",
    "        titles = titles.split()\n",
    "        for words in titles:\n",
    "            if words not in stop_words:\n",
    "                go_words.append(words)\n",
    "    return go_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Article Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the topic modelling section of this project, I will be using a Latent Semantic Analysis model. Because I am interested in how topics are similar or different between the True and Fake news articles, I have first split up my dataset again to look at the topics individually and comparatively. This split is only used for the topic modelling analysis. My classifier will be trained on the whole dataset created earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the data so I can look at the Fake news titles in comparison to the True news titles\n",
    "fake_news_titles = combined_news_dataset.loc[combined_news_dataset['label'] == 0, 'title']\n",
    "true_news_titles = combined_news_dataset.loc[combined_news_dataset['label'] == 1, 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up TF-IDF to use my tokenizer as defined above\n",
    "#the modeling of my LSA model is based on our Lecture 2.2 material\n",
    "tfidf_vectoriser = TfidfVectorizer(tokenizer=my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing TruncatedSVD to use later\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling the Fake News Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-d7e0d61d529a>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  article_titles = combined_news_dataset[\"title\"].str.replace(\"U.S.\", \"unitedstates\", case = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1174, 5996)\n"
     ]
    }
   ],
   "source": [
    "#applying my tokenizer to the dataset and using it to create a variable holding all tokenized words\n",
    "#printing out the shape of the dataset and the number of tokenized words\n",
    "\n",
    "fake_tfidf = tfidf_vectoriser.fit_transform(fake_news_titles)\n",
    "fake_vocab = tfidf_vectoriser.get_feature_names_out()\n",
    "fake_tfidf_df = pd.DataFrame(fake_tfidf.todense(), columns = fake_vocab)\n",
    "print(fake_tfidf.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tfidf_df = fake_tfidf_df - fake_tfidf_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up variables and number of topics for use later\n",
    "\n",
    "fake_num_topics = 5\n",
    "pd.options.display.max_columns=fake_num_topics\n",
    "fake_labels = ['topic{}'.format(i) for i in range(fake_num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/stem/lib/python3.9/site-packages/sklearn/decomposition/_truncated_svd.py:268: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "/opt/miniconda3/envs/stem/lib/python3.9/site-packages/sklearn/decomposition/_truncated_svd.py:268: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    }
   ],
   "source": [
    "#applying TruncatedSVD to generate a matrix from the data\n",
    "\n",
    "fake_svd = TruncatedSVD(n_components = fake_num_topics, n_iter = 100) \n",
    "fake_svd_topic_vectors = fake_svd.fit_transform(fake_tfidf_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tounitedstatespromises</th>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sh</th>\n",
       "      <td>-0.006167</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>-0.016598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>given</th>\n",
       "      <td>-0.002398</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meryl</th>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ite</th>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stores</th>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apologizes</th>\n",
       "      <td>-0.002398</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaw</th>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spirit</th>\n",
       "      <td>-0.002398</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baiting</th>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          topic0    topic1    topic2    topic3    topic4\n",
       "tounitedstatespromises -0.001199 -0.000019  0.000760  0.000306  0.000567\n",
       "sh                     -0.006167  0.000932  0.003472  0.016210 -0.016598\n",
       "given                  -0.002398 -0.000039  0.001521  0.000611  0.001135\n",
       "meryl                  -0.001199 -0.000019  0.000760  0.000306  0.000567\n",
       "ite                    -0.001199 -0.000019  0.000760  0.000306  0.000567\n",
       "stores                 -0.001199 -0.000019  0.000760  0.000306  0.000567\n",
       "apologizes             -0.002398 -0.000039  0.001521  0.000611  0.001135\n",
       "jaw                    -0.001199 -0.000019  0.000760  0.000306  0.000567\n",
       "spirit                 -0.002398 -0.000039  0.001521  0.000611  0.001135\n",
       "baiting                -0.001199 -0.000019  0.000760  0.000306  0.000567"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the topic_weights variable and confirming the topic weights is set up properly by testing it on a sample of the data.\n",
    "\n",
    "fake_topic_weights = pd.DataFrame(fake_svd.components_.T, index=fake_vocab, columns=fake_labels)\n",
    "fake_topic_weights.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___topic 0___\n",
      "['debate' 'voters' 'gop' 'house' 'tweets' 'north' 'eu' 'chief' 'deal'\n",
      " 'bill' 'clinton' 'media' 'vote' 'black' 'president' 'says' 'watch'\n",
      " 'white' 'obama' 'unitedstates']\n",
      "___topic 1___\n",
      "['lives' 'american' 'general' 'committee' 'wife' 'korean' 'media' 'obama'\n",
      " 'donald' 'foreign' 'two' 'korea' 'police' 'anti' 'hillary' 'president'\n",
      " 'house' 'new' 'watch' '000']\n",
      "___topic 2___\n",
      "['world' 'race' 'wall' 'rights' 'help' 'stop' 'gets' 'donald' 'clinton'\n",
      " 'people' 'syria' 'muslim' 'campaign' 'war' 'speech' 'news' 'republican'\n",
      " 'new' 'obama' 'court']\n",
      "___topic 3___\n",
      "['report' 'fight' 'south' 'woman' 'man' 'poll' 'tells' 'americans'\n",
      " 'minister' 'sanctions' 'putin' 'tweets' 'north' 'white' 'obama' 'donald'\n",
      " 'republican' 'news' 'video' 'unitedstates']\n",
      "___topic 4___\n",
      "['healthcare' 'claims' 'saudi' 'department' 'killed' 'pm' 'week' 'left'\n",
      " 'brexit' 'probe' 'gop' 'unitedstates' 'china' '000' 'trump' 'video'\n",
      " 'bill' 'clinton' '111' 'court']\n"
     ]
    }
   ],
   "source": [
    "#pull out 20 terms from the top 5 topics as defined earlier\n",
    "\n",
    "num_terms = 20\n",
    "for i in range(fake_num_topics):\n",
    "    print(\"___topic \" + str(i) + \"___\")\n",
    "    fake_topicName = \"topic\" + str(i)\n",
    "    fake_weightedlist = fake_topic_weights.get(fake_topicName).sort_values()[-num_terms:]\n",
    "    print(fake_weightedlist.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling the True News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-d7e0d61d529a>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  article_titles = combined_news_dataset[\"title\"].str.replace(\"U.S.\", \"unitedstates\", case = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1071, 5996)\n"
     ]
    }
   ],
   "source": [
    "#applying my tokenizer to the dataset and using it to create a variable holding all tokenized words\n",
    "#printing out the shape of the dataset and the number of tokenized words\n",
    "\n",
    "true_tfidf = tfidf_vectoriser.fit_transform(true_news_titles)\n",
    "true_vocab = tfidf_vectoriser.get_feature_names_out()\n",
    "true_tfidf_df = pd.DataFrame(true_tfidf.todense(), columns = true_vocab)\n",
    "print(true_tfidf.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tfidf_df = true_tfidf_df - true_tfidf_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up variables and number of topics for use later\n",
    "\n",
    "true_num_topics = 5\n",
    "pd.options.display.max_columns=true_num_topics\n",
    "true_labels = ['topic{}'.format(i) for i in range(true_num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/stem/lib/python3.9/site-packages/sklearn/decomposition/_truncated_svd.py:268: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "/opt/miniconda3/envs/stem/lib/python3.9/site-packages/sklearn/decomposition/_truncated_svd.py:268: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    }
   ],
   "source": [
    "#applying TruncatedSVD to generate a matrix from the data\n",
    "\n",
    "true_svd = TruncatedSVD(n_components = true_num_topics, n_iter = 100) \n",
    "true_svd_topic_vectors = true_svd.fit_transform(true_tfidf_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stresses</th>\n",
       "      <td>-0.002484</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voted</th>\n",
       "      <td>-0.003109</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>investigators</th>\n",
       "      <td>-0.002484</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unitedstatesretired</th>\n",
       "      <td>-0.001242</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bewilder</th>\n",
       "      <td>-0.001242</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>changing</th>\n",
       "      <td>-0.002484</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eviscerates</th>\n",
       "      <td>-0.001242</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tried</th>\n",
       "      <td>-0.004968</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pepper</th>\n",
       "      <td>-0.001242</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fees</th>\n",
       "      <td>-0.001242</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       topic0    topic1    topic2    topic3    topic4\n",
       "stresses            -0.002484  0.000099  0.000017  0.000017  0.000026\n",
       "voted               -0.003109  0.000004  0.000008  0.000036 -0.000018\n",
       "investigators       -0.002484  0.000099  0.000017  0.000017  0.000026\n",
       "unitedstatesretired -0.001242  0.000049  0.000008  0.000008  0.000013\n",
       "bewilder            -0.001242  0.000049  0.000008  0.000008  0.000013\n",
       "changing            -0.002484  0.000099  0.000017  0.000017  0.000026\n",
       "eviscerates         -0.001242  0.000049  0.000008  0.000008  0.000013\n",
       "tried               -0.004968  0.000197  0.000034  0.000033  0.000053\n",
       "pepper              -0.001242  0.000049  0.000008  0.000008  0.000013\n",
       "fees                -0.001242  0.000049  0.000008  0.000008  0.000013"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the topic_weights variable and confirming the topic weights is set up properly by testing it on a sample of the data.\n",
    "\n",
    "true_topic_weights = pd.DataFrame(true_svd.components_.T, index=true_vocab, columns=true_labels)\n",
    "true_topic_weights.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___topic 0___\n",
      "['factbox' 'runitedstatesan' 'bill' 'gop' 'house' 'deal' 'eu' 'chief'\n",
      " 'tweets' 'clinton' 'north' 'vote' 'black' 'media' 'says' 'president'\n",
      " 'obama' 'white' 'watch' 'unitedstates']\n",
      "___topic 1___\n",
      "['says' 'gets' 'attack' 'runitedstatesan' 'factbox' 'obama' 'new'\n",
      " 'hillary' 'media' '05' 'court' '104' '100k' '11' 'video' 'news'\n",
      " 'republican' '108' '1' '000']\n",
      "___topic 2___\n",
      "['latest' 'want' 'students' 'action' 'general' 'islamic' 'korean' 'order'\n",
      " 'years' 'way' 'gets' 'two' 'foreign' 'hillary' 'unitedstates' 'video'\n",
      " 'trump' '05' '100k' '1']\n",
      "___topic 3___\n",
      "['campaign' 'speech' 'war' 'syria' 'people' 'new' 'donald' 'court'\n",
      " 'hillary' 'republican' 'news' '02' 'video' '04' '05' '100k' '1' '106'\n",
      " '10' '10th']\n",
      "___topic 4___\n",
      "['war' 'speech' 'campaign' 'tweets' 'north' 'clinton' 'vote' 'black'\n",
      " 'gets' 'obama' 'says' 'watch' 'white' 'trump' '04' '000' '02' '05' '100'\n",
      " '100k']\n"
     ]
    }
   ],
   "source": [
    "#pull out 20 terms from the top 5 topics as defined earlier\n",
    "\n",
    "num_terms = 20\n",
    "for i in range(true_num_topics):\n",
    "    print(\"___topic \" + str(i) + \"___\")\n",
    "    true_topicName = \"topic\" + str(i)\n",
    "    true_weightedlist = true_topic_weights.get(true_topicName).sort_values()[-num_terms:]\n",
    "    print(true_weightedlist.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Classifier based on Article Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have elected to use an LSTM classifier on my data due to the ease and speed of training. Due to time constraints, I have used the classifying method as laid out in NLP Lecture 5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. House committee 'may reconsider' WHO canc...</td>\n",
       "      <td>LONDON (Reuters) - U.S. congressional committe...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 8, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Congratulations': EU moves to Brexit phase tw...</td>\n",
       "      <td>BRUSSELS (Reuters) - The European Union agreed...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 15, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White House aides told to preserve materials i...</td>\n",
       "      <td>WASHINGTON (Reuters) - The White House counsel...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>March 2, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. 'very concerned' by violence around Iraq'...</td>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. State Departme...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 16, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama's move on gender pay gap seen as first s...</td>\n",
       "      <td>NEW YORK (Reuters) - Advocates fighting to clo...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>February 5, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  U.S. House committee 'may reconsider' WHO canc...   \n",
       "1  'Congratulations': EU moves to Brexit phase tw...   \n",
       "2  White House aides told to preserve materials i...   \n",
       "3  U.S. 'very concerned' by violence around Iraq'...   \n",
       "4  Obama's move on gender pay gap seen as first s...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  LONDON (Reuters) - U.S. congressional committe...  politicsNews   \n",
       "1  BRUSSELS (Reuters) - The European Union agreed...     worldnews   \n",
       "2  WASHINGTON (Reuters) - The White House counsel...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - The U.S. State Departme...     worldnews   \n",
       "4  NEW YORK (Reuters) - Advocates fighting to clo...  politicsNews   \n",
       "\n",
       "                 date  label  \n",
       "0   December 8, 2017       1  \n",
       "1  December 15, 2017       1  \n",
       "2      March 2, 2017       1  \n",
       "3   October 16, 2017       1  \n",
       "4   February 5, 2016       1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at my dataset\n",
    "combined_news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. House committee 'may reconsider' WHO canc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Congratulations': EU moves to Brexit phase tw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White House aides told to preserve materials i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. 'very concerned' by violence around Iraq'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama's move on gender pay gap seen as first s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  label\n",
       "0  U.S. House committee 'may reconsider' WHO canc...      1\n",
       "1  'Congratulations': EU moves to Brexit phase tw...      1\n",
       "2  White House aides told to preserve materials i...      1\n",
       "3  U.S. 'very concerned' by violence around Iraq'...      1\n",
       "4  Obama's move on gender pay gap seen as first s...      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping unnecessary aspects of our dataset for training the model\n",
    "classifying_dataset = combined_news_dataset.drop([\"text\", \"subject\", \"date\"], axis=1)\n",
    "classifying_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelling my training model after NLP Lecture 5.1.2\n",
    "from nltk.tokenize.casual import casual_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading the GoogleNews Vectors to assist with vectorizing my data\n",
    "\n",
    "embeddings_file = \"GoogleNews-vectors-negative300.bin\"\n",
    "wv = KeyedVectors.load_word2vec_format(embeddings_file, binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a tokenizer and vectorizer\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=200000)\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    vectorized_data = []\n",
    "    for sample in dataset:\n",
    "        tokens = casual_tokenize(sample)\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass  # No matching token in the Google w2v vocab\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to make all vectors the same length and shape\n",
    "\n",
    "def pad_trunc(data, maxlen):\n",
    "    new_data = []\n",
    "\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    " \n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titles_dataset = classifying_dataset.sample(frac = 1) \n",
    "features = tokenize_and_vectorize(classifying_dataset[\"title\"])\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, classifying_dataset[\"label\"], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "embedding_dims = 300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-6dd20099d2f7>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(x_train).shape,np.array(x_test).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1571,), (674,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the shape of my training and test sets\n",
    "np.array(x_train).shape,np.array(x_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the pad function to the training and test sets\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1571, 50, 300), (674, 50, 300))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the new shape of the training and test sets\n",
    "np.array(x_train).shape,np.array(x_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32       \n",
    "num_neurons = 10     \n",
    "epochs = 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 12,451\n",
      "Trainable params: 12,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#setting up the model to be trained\n",
    "\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, SimpleRNN, LSTM\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=False, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50/50 [==============================] - 186s 4s/step - loss: 0.6059 - accuracy: 0.7638 - val_loss: 0.4080 - val_accuracy: 0.9896\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 186s 4s/step - loss: 0.3341 - accuracy: 0.9866 - val_loss: 0.2534 - val_accuracy: 0.9866\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 186s 4s/step - loss: 0.2234 - accuracy: 0.9860 - val_loss: 0.1771 - val_accuracy: 0.9866\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "#training the LSTM model on my dataset\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "model_structure = model.to_json()\n",
    "with open(\"simplernn_model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"simplernn_weights2.h5\")\n",
    "print('Model saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
